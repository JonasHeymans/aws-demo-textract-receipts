{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Textract - Receipts Demo\n",
    "\n",
    "In the following notebook, we will examine the use of Amazon Textract in order to perform optical character recognition (OCR), and then use Amazon Machine Learning to organize the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Imports\n",
    "\n",
    "These are the libraries which we will require in order to complete different types of operations.\n",
    "\n",
    "Just a quick note, you may need to use ```nltk.download()``` in order to download the correct libraries and additional packages required to perform text tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk \n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "from datetime import datetime\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.stem import PorterStemmer \n",
    "import string\n",
    "import pickle\n",
    "from nltk.corpus import stopwords \n",
    "from itertools import combinations \n",
    "import operator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Set-up\n",
    "Configure all global constants here, these are variables which will remain constant throughout the execution of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'aws_region' :  'us-east-1',\n",
    "    'bucket_name': 'aws-demo-receipts',\n",
    "    'prefix' : 'preprocessed_reviews', #only use this if you want to have your files in a folder \n",
    "    'dataset_path_s3' : 'receipts_data/',\n",
    "    'training_data_prefix_s3' : 'training_data/',\n",
    "    'output_data_s3' : 'output_data/',\n",
    "    'dataset_zipped_filename': 'large-receipt-image-dataset-SRD.zip',\n",
    "    'dataset_zipped_path':'data_zipped/',\n",
    "    'tmp_folder':'tmp/',\n",
    "    'stopwords_custom': 'stopwords_custom.txt'\n",
    "   \n",
    "}\n",
    "\n",
    "global_vars = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Setting up the environment involves ensuring all the corret session and IAM roles are configured. We also need to ensure the correct region and bucket is made available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket already exists\n"
     ]
    }
   ],
   "source": [
    "def setup_env(configs, global_vars):\n",
    "   \n",
    "    sess = sagemaker.Session()\n",
    "    \n",
    "    role = get_execution_role()\n",
    "\n",
    "    AWS_REGION = configs['aws_region']\n",
    "    s3 = boto3.resource('s3')\n",
    "\n",
    "    s3_bucket = s3.Bucket(configs['bucket_name'])\n",
    "\n",
    "    if s3_bucket.creation_date == None:\n",
    "    # create S3 bucket because it does not exist yet\n",
    "        print('Creating S3 bucket {}.'.format(bucket))\n",
    "        resp = s3.create_bucket(\n",
    "            ACL='private',\n",
    "            Bucket=bucket\n",
    "        )\n",
    "    else:\n",
    "        print('Bucket already exists')\n",
    "        \n",
    "    global_vars['role'] = role\n",
    "    global_vars['sess'] = sess\n",
    "    global_vars['s3'] = s3\n",
    "    global_vars['s3_bucket'] = s3_bucket\n",
    "    \n",
    "    \n",
    "    #set up textract\n",
    "    textract = boto3.client('textract')\n",
    "    \n",
    "    global_vars['textract'] = textract\n",
    "\n",
    "    \n",
    "    return global_vars\n",
    "\n",
    "global_vars = setup_env(configs, global_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_nlp(configs, global_vars):\n",
    "    \n",
    "    nlp = en_core_web_lg.load()\n",
    "    global_vars['nlp'] = nlp\n",
    "    stop_words = []\n",
    "    with open(configs['stopwords_custom'], 'r') as file:\n",
    "        for line in file:\n",
    "            stop_words.append(line.strip())\n",
    "    global_vars['stop_words'] = stop_words\n",
    "    return global_vars\n",
    "\n",
    "global_vars = setup_nlp(configs, global_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data and Create Manifest\n",
    "\n",
    "Here we are goin to download the data to our bucket if it does not already exist. This dataset is a pre-compiled dataset of car images which contain 2 typers:\n",
    "\n",
    "- Whole (e.g. without damage)\n",
    "- Damaged (e.g. those with damage)\n",
    "\n",
    "The following create_dataset method will first download the Zip of the data, and then unpack it to the bucket named in the global constants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(configs, global_vars):\n",
    "    \n",
    "    dataset_key = configs['dataset_path_s3']\n",
    "    s3_bucket = global_vars['s3_bucket']\n",
    "    objs = list(s3_bucket.objects.filter(Prefix=dataset_key))\n",
    "\n",
    "    if len(objs) > 0 and objs[0].key == configs['dataset_zipped_filename']:\n",
    "        exists = True\n",
    "        print('{} Already Exists, No need to copy to S3 or Unzip'.format(dataset_key) )\n",
    "    else:\n",
    "        exists = False\n",
    "    \n",
    "    if not exists:\n",
    "        \n",
    "        data_path = configs['dataset_zipped_path'] + configs['dataset_zipped_filename']\n",
    "        data_file_s3 = configs['dataset_path_s3'] + configs['dataset_zipped_filename']\n",
    "        s3_bucket.upload_file(data_path, data_file_s3)\n",
    "\n",
    "        s3 = boto3.client('s3', use_ssl=False)\n",
    "        Key_unzip = configs['dataset_path_s3']\n",
    "\n",
    "    \n",
    "        s3_resource = boto3.resource('s3')\n",
    "        #Now create zip object one by one, this below is for 1st file in file_list\n",
    "        zip_obj = s3_resource.Object(bucket_name=bucket_name, key=data_file_s3)\n",
    "        \n",
    "        print('Unpacking {}\\n'.format(data_file_s3))\n",
    "        print (zip_obj)\n",
    "        buffer = BytesIO(zip_obj.get()[\"Body\"].read())\n",
    "        z = zipfile.ZipFile(buffer)\n",
    "        for filename in z.namelist():\n",
    "            file_info = z.getinfo(filename)\n",
    "            s3_resource.meta.client.upload_fileobj(\n",
    "                z.open(filename),\n",
    "                Bucket=bucket_name,\n",
    "                Key=Key_unzip + f'{filename}')\n",
    "            \n",
    "        \n",
    "    \n",
    "# create_dataset(bucket_name, s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receipts_data/\n",
      "receipts_data/1000-receipt.jpg\n",
      "receipts_data/1001-receipt.jpg\n",
      "receipts_data/1002-receipt.jpg\n",
      "receipts_data/1003-receipt.jpg\n",
      "receipts_data/1004-receipt.jpg\n",
      "receipts_data/1005-receipt.jpg\n",
      "receipts_data/1006-receipt.jpg\n",
      "receipts_data/1007-receipt.jpg\n",
      "receipts_data/1008-receipt.jpg\n",
      "receipts_data/1009-receipt.jpg\n",
      "receipts_data/1010-receipt.jpg\n",
      "receipts_data/1011-receipt.jpg\n",
      "receipts_data/1012-receipt.jpg\n",
      "receipts_data/1013-receipt.jpg\n",
      "receipts_data/1014-receipt.jpg\n",
      "receipts_data/1015-receipt.jpg\n",
      "receipts_data/1016-receipt.jpg\n",
      "receipts_data/1017-receipt.jpg\n",
      "receipts_data/1018-receipt.jpg\n",
      "receipts_data/1019-receipt.jpg\n",
      "receipts_data/1020-receipt.jpg\n",
      "receipts_data/1021-receipt.jpg\n",
      "receipts_data/1022-receipt.jpg\n",
      "receipts_data/1023-receipt.jpg\n",
      "receipts_data/1024-receipt.jpg\n",
      "receipts_data/1025-receipt.jpg\n",
      "receipts_data/1026-receipt.jpg\n",
      "receipts_data/1027-receipt.jpg\n",
      "receipts_data/1028-receipt.jpg\n",
      "receipts_data/1029-receipt.jpg\n",
      "receipts_data/1030-receipt.jpg\n",
      "receipts_data/1031-receipt.jpg\n",
      "receipts_data/1032-receipt.jpg\n",
      "receipts_data/1033-receipt.jpg\n",
      "receipts_data/1034-receipt.jpg\n",
      "receipts_data/1035-receipt.jpg\n",
      "receipts_data/1036-receipt.jpg\n",
      "receipts_data/1037-receipt.jpg\n",
      "receipts_data/1038-receipt.jpg\n",
      "receipts_data/1039-receipt.jpg\n",
      "receipts_data/1040-receipt.jpg\n",
      "receipts_data/1041-receipt.jpg\n",
      "receipts_data/1042-receipt.jpg\n",
      "receipts_data/1043-receipt.jpg\n",
      "receipts_data/1044-receipt.jpg\n",
      "receipts_data/1045-receipt.jpg\n",
      "receipts_data/1046-receipt.jpg\n",
      "receipts_data/1047-receipt.jpg\n",
      "receipts_data/1048-receipt.jpg\n",
      "receipts_data/1049-receipt.jpg\n",
      "receipts_data/1050-receipt.jpg\n",
      "receipts_data/1051-receipt.jpg\n",
      "receipts_data/1052-receipt.jpg\n",
      "receipts_data/1053-receipt.jpg\n",
      "receipts_data/1054-receipt.jpg\n",
      "receipts_data/1055-receipt.jpg\n",
      "receipts_data/1056-receipt.jpg\n",
      "receipts_data/1057-receipt.jpg\n",
      "receipts_data/1058-receipt.jpg\n",
      "receipts_data/1059-receipt.jpg\n",
      "receipts_data/1060-receipt.jpg\n",
      "receipts_data/1061-receipt.jpg\n",
      "receipts_data/1062-receipt.jpg\n",
      "receipts_data/1063-receipt.jpg\n",
      "receipts_data/1064-receipt.jpg\n",
      "receipts_data/1065-receipt.jpg\n",
      "receipts_data/1066-receipt.jpg\n",
      "receipts_data/1067-receipt.jpg\n",
      "receipts_data/1068-receipt.jpg\n",
      "receipts_data/1069-receipt.jpg\n",
      "receipts_data/1070-receipt.jpg\n",
      "receipts_data/1071-receipt.jpg\n",
      "receipts_data/1072-receipt.jpg\n",
      "receipts_data/1073-receipt.jpg\n",
      "receipts_data/1074-receipt.jpg\n",
      "receipts_data/1075-receipt.jpg\n",
      "receipts_data/1076-receipt.jpg\n",
      "receipts_data/1077-receipt.jpg\n",
      "receipts_data/1078-receipt.jpg\n",
      "receipts_data/1079-receipt.jpg\n",
      "receipts_data/1080-receipt.jpg\n",
      "receipts_data/1081-receipt.jpg\n",
      "receipts_data/1082-receipt.jpg\n",
      "receipts_data/1083-receipt.jpg\n",
      "receipts_data/1084-receipt.jpg\n",
      "receipts_data/1085-receipt.jpg\n",
      "receipts_data/1086-receipt.jpg\n",
      "receipts_data/1087-receipt.jpg\n",
      "receipts_data/1088-receipt.jpg\n",
      "receipts_data/1089-receipt.jpg\n",
      "receipts_data/1090-receipt.jpg\n",
      "receipts_data/1091-receipt.jpg\n",
      "receipts_data/1092-receipt.jpg\n",
      "receipts_data/1093-receipt.jpg\n",
      "receipts_data/1094-receipt.jpg\n",
      "receipts_data/1095-receipt.jpg\n",
      "receipts_data/1096-receipt.jpg\n",
      "receipts_data/1097-receipt.jpg\n",
      "receipts_data/1098-receipt.jpg\n",
      "receipts_data/1099-receipt.jpg\n",
      "receipts_data/1100-receipt.jpg\n",
      "receipts_data/1101-receipt.jpg\n",
      "receipts_data/1102-receipt.jpg\n",
      "receipts_data/1103-receipt.jpg\n",
      "receipts_data/1104-receipt.jpg\n",
      "receipts_data/1105-receipt.jpg\n",
      "receipts_data/1106-receipt.jpg\n",
      "receipts_data/1107-receipt.jpg\n",
      "receipts_data/1108-receipt.jpg\n",
      "receipts_data/1109-receipt.jpg\n",
      "receipts_data/1110-receipt.jpg\n",
      "receipts_data/1111-receipt.jpg\n",
      "receipts_data/1112-receipt.jpg\n",
      "receipts_data/1113-receipt.jpg\n",
      "receipts_data/1114-receipt.jpg\n",
      "receipts_data/1115-receipt.jpg\n",
      "receipts_data/1116-receipt.jpg\n",
      "receipts_data/1117-receipt.jpg\n",
      "receipts_data/1118-receipt.jpg\n",
      "receipts_data/1119-receipt.jpg\n",
      "receipts_data/1120-receipt.jpg\n",
      "receipts_data/1121-receipt.jpg\n",
      "receipts_data/1122-receipt.jpg\n",
      "receipts_data/1123-receipt.jpg\n",
      "receipts_data/1124-receipt.jpg\n",
      "receipts_data/1125-receipt.jpg\n",
      "receipts_data/1126-receipt.jpg\n",
      "receipts_data/1127-receipt.jpg\n",
      "receipts_data/1128-receipt.jpg\n",
      "receipts_data/1129-receipt.jpg\n",
      "receipts_data/1130-receipt.jpg\n",
      "receipts_data/1131-receipt.jpg\n",
      "receipts_data/1132-receipt.jpg\n",
      "receipts_data/1133-receipt.jpg\n",
      "receipts_data/1134-receipt.jpg\n",
      "receipts_data/1135-receipt.jpg\n",
      "receipts_data/1136-receipt.jpg\n",
      "receipts_data/1137-receipt.jpg\n",
      "receipts_data/1138-receipt.jpg\n",
      "receipts_data/1139-receipt.jpg\n",
      "receipts_data/1140-receipt.jpg\n",
      "receipts_data/1141-receipt.jpg\n",
      "receipts_data/1142-receipt.jpg\n",
      "receipts_data/1143-receipt.jpg\n",
      "receipts_data/1144-receipt.jpg\n",
      "receipts_data/1145-receipt.jpg\n",
      "receipts_data/1146-receipt.jpg\n",
      "receipts_data/1147-receipt.jpg\n",
      "receipts_data/1148-receipt.jpg\n",
      "receipts_data/1149-receipt.jpg\n",
      "receipts_data/1150-receipt.jpg\n",
      "receipts_data/1151-receipt.jpg\n",
      "receipts_data/1152-receipt.jpg\n",
      "receipts_data/1153-receipt.jpg\n",
      "receipts_data/1154-receipt.jpg\n",
      "receipts_data/1155-receipt.jpg\n",
      "receipts_data/1156-receipt.jpg\n",
      "receipts_data/1157-receipt.jpg\n",
      "receipts_data/1158-receipt.jpg\n",
      "receipts_data/1159-receipt.jpg\n",
      "receipts_data/1160-receipt.jpg\n",
      "receipts_data/1161-receipt.jpg\n",
      "receipts_data/1162-receipt.jpg\n",
      "receipts_data/1163-receipt.jpg\n",
      "receipts_data/1164-receipt.jpg\n",
      "receipts_data/1165-receipt.jpg\n",
      "receipts_data/1166-receipt.jpg\n",
      "receipts_data/1167-receipt.jpg\n",
      "receipts_data/1168-receipt.jpg\n",
      "receipts_data/1169-receipt.jpg\n",
      "receipts_data/1170-receipt.jpg\n",
      "receipts_data/1171-receipt.jpg\n",
      "receipts_data/1172-receipt.jpg\n",
      "receipts_data/1173-receipt.jpg\n",
      "receipts_data/1174-receipt.jpg\n",
      "receipts_data/1175-receipt.jpg\n",
      "receipts_data/1176-receipt.jpg\n",
      "receipts_data/1177-receipt.jpg\n",
      "receipts_data/1178-receipt.jpg\n",
      "receipts_data/1179-receipt.jpg\n",
      "receipts_data/1180-receipt.jpg\n",
      "receipts_data/1181-receipt.jpg\n",
      "receipts_data/1182-receipt.jpg\n",
      "receipts_data/1183-receipt.jpg\n",
      "receipts_data/1184-receipt.jpg\n",
      "receipts_data/1185-receipt.jpg\n",
      "receipts_data/1186-receipt.jpg\n",
      "receipts_data/1187-receipt.jpg\n",
      "receipts_data/1188-receipt.jpg\n",
      "receipts_data/1189-receipt.jpg\n",
      "receipts_data/1190-receipt.jpg\n",
      "receipts_data/1191-receipt.jpg\n",
      "receipts_data/1192-receipt.jpg\n",
      "receipts_data/1193-receipt.jpg\n",
      "receipts_data/1194-receipt.jpg\n",
      "receipts_data/1195-receipt.jpg\n",
      "receipts_data/1196-receipt.jpg\n",
      "receipts_data/1197-receipt.jpg\n",
      "receipts_data/1198-receipt.jpg\n",
      "receipts_data/1199-receipt.jpg\n"
     ]
    }
   ],
   "source": [
    "def test_textract(configs, global_vars, show_image = False):\n",
    "    \n",
    "    #first download a temp document\n",
    "    tmp_folder = configs['tmp_folder']\n",
    "    try:\n",
    "        os.mkdir(tmp_folder)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    s3_bucket = global_vars['s3_bucket']\n",
    "    s3 = global_vars['s3']\n",
    "    \n",
    "    s3_path = configs['dataset_path_s3']\n",
    "    objs = list(s3_bucket.objects.filter(Prefix=s3_path))\n",
    "    \n",
    "    files_data = {}\n",
    "    for entry in objs:\n",
    "        print(entry.key)\n",
    "        file_data = []\n",
    "        if 'jpg' in entry.key:\n",
    "            local_path = tmp_folder + 'tmp.jpg'\n",
    "            s3.Bucket(configs['bucket_name']).download_file(str(entry.key), local_path)  \n",
    "            \n",
    "            with open(local_path, 'rb') as document:\n",
    "                imageBytes = bytearray(document.read())\n",
    "            \n",
    "            response = global_vars['textract'].detect_document_text(Document={'Bytes': imageBytes})\n",
    "\n",
    "            if show_image:\n",
    "                raw_img = mpimg.imread(local_path)\n",
    "                height, width, depth = raw_img.shape\n",
    "                dpi = 80\n",
    "                figsize = width / float(dpi), height / float(dpi)\n",
    "                plt.figure(figsize = figsize)\n",
    "                plt.imshow(raw_img, aspect='auto')\n",
    "\n",
    "            files_data[entry.key] =  response\n",
    "            \n",
    "    return files_data\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "textract_data = test_textract(configs, global_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Method if you want to resume a session if the kernal dies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_save_record_meta_data(textract_data=None, load_or_save='load'):\n",
    "    \n",
    "    tmp_filename = 'tmp.pickle'\n",
    "    if load_or_save == 'load':\n",
    "        with open(tmp_filename, 'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "        return data\n",
    "\n",
    "        \n",
    "    if load_or_save == 'save':\n",
    "        with open(tmp_filename, 'wb') as handle:\n",
    "            pickle.dump(textract_data, handle, protocol=pickle.HIGHEST_PROTOCOL)  \n",
    "        print('Saved to {}'.format(tmp_filename))\n",
    "        return textract_data\n",
    "    \n",
    "textract_data = load_or_save_record_meta_data('', 'load')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records 200\n"
     ]
    }
   ],
   "source": [
    "def inspect_dataset(textract_data):\n",
    "    \n",
    "    print('Total Records {}'.format(len(textract_data)))\n",
    "\n",
    "inspect_dataset(textract_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich Dataset\n",
    "\n",
    "The Following methods will be used to enrich each of the records with additional information.\n",
    "\n",
    "We're going to use two libraries to perform our enrichment, ```NLTK```, and ```Spacy```.\n",
    "\n",
    "#### Part of Speech Tagging (POS)\n",
    "\n",
    "In the world of Natural Language Processing (NLP), the most basic models are based on Bag of Words. But such models fail to capture the syntactic relations between words. ... POS tagging is the process of marking up a word in a corpus to a corresponding part of a speech tag, based on its context and definition.\n",
    "\n",
    "For Part of Speak Tagging, we're using NLTK. More details can be found here:\n",
    "\n",
    "https://www.nltk.org/api/nltk.tag.html\n",
    "\n",
    "The tags which NLTK uses are as follows:\n",
    "\n",
    " - CD - cardinal numbers\n",
    " - AT -Articles\n",
    " - JJ - Adjectives\n",
    " - NN Nouns\n",
    " - NNS - Plural Nouns\n",
    " - RB - adverbs\n",
    " - VGB - gerunds\n",
    " - VBD - past tense verbs\n",
    " \n",
    "#### Named Entity Recognition (NER)\n",
    "\n",
    "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
    "\n",
    "For our NER, spaCy features an extremely fast statistical entity recognition system, that assigns labels to contiguous spans of tokens. The default model identifies a variety of named and numeric entities, including companies, locations, organizations and products.\n",
    "\n",
    "For More information on Spacey, please take a read here:\n",
    "\n",
    "\n",
    "\n",
    "The Tags which Space use are as follows:\n",
    "\n",
    " - PERSON\tPeople, including fictional.\n",
    " - NORP\tNationalities or religious or political groups.\n",
    " - FAC\tBuildings, airports, highways, bridges, etc.\n",
    " - ORG\tCompanies, agencies, institutions, etc.\n",
    " - GPE\tCountries, cities, states.\n",
    " - LOC\tNon-GPE locations, mountain ranges, bodies of water.\n",
    " - PRODUCT\tObjects, vehicles, foods, etc. (Not services.)\n",
    " - EVENT\tNamed hurricanes, battles, wars, sports events, etc.\n",
    " - WORK_OF_ART\tTitles of books, songs, etc.\n",
    " - LAW\tNamed documents made into laws.\n",
    " - LANGUAGE\tAny named language.\n",
    " - DATE\tAbsolute or relative dates or periods.\n",
    " - TIME\tTimes smaller than a day.\n",
    " - PERCENT\tPercentage, including ”%“.\n",
    " - MONEY\tMonetary values, including unit.\n",
    " - UANTITY\tMeasurements, as of weight or distance.\n",
    " - ORDINAL\t“first”, “second”, etc.\n",
    " - CARDINAL\tNumerals that do not fall under another type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_textract_text(data, print_output= True):\n",
    "    \n",
    "    currencies = ['$',' £','€']\n",
    "    \n",
    "    values = []\n",
    "    currency_values = []\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    UPPER_BOUND = 2000.0\n",
    "    \n",
    "    for line in data:\n",
    "        #first see if it can be recognised as a value:\n",
    "        if any(ext in line for ext in currencies):\n",
    "            currency_values.append(line)\n",
    "#             print(line)\n",
    "        else:\n",
    "            try:\n",
    "                value = float(line)\n",
    "                values.append(value)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        #now P O W taggig\n",
    "        wrds = nltk.word_tokenize(line)\n",
    "        words = words + wrds\n",
    "    \n",
    "    #nltk POS tagging\n",
    "    nltk_tagged_words = nltk.pos_tag(words)\n",
    "    \n",
    "    #perform Spacy Analysis:\n",
    "    spacey_enriched_words = spacey_enricher(global_vars, data)\n",
    "    \n",
    "    verbs = []\n",
    "    nouns = []\n",
    "    cds = []\n",
    "    products = []\n",
    "    locations = []\n",
    "    organisations = []\n",
    "    dates = []\n",
    "    for entry in nltk_tagged_words:\n",
    "#         print(entry)\n",
    "        k = entry[0]\n",
    "\n",
    "        v = entry[1]\n",
    "        if v == 'NN' or v == 'NNS':\n",
    "            nouns.append(k)\n",
    "            \n",
    "        if v == 'VBD' or v == 'VGB':\n",
    "            verbs.append(k)\n",
    "        \n",
    "        if v == 'CD':\n",
    "            cds.append(k)  \n",
    "            \n",
    "           \n",
    "    #process SPACEY results\n",
    "    for entry in spacey_enriched_words.ents:\n",
    "        if entry.label_ is 'PRODUCT':\n",
    "            products.append(entry.text)\n",
    "        \n",
    "        if entry.label_ is 'LOC':\n",
    "            locations.append(entry.text)\n",
    "        \n",
    "        if entry.label_ is 'ORG':\n",
    "            organisations.append(entry.text)\n",
    "            \n",
    "#         if entry.label_ is 'DATE':\n",
    "#             dates.append(entry.text)\n",
    "                    \n",
    "    #calculate max cardinal values\n",
    "    cds_float = []\n",
    "    for value in cds:\n",
    "        #firsr monetary values\n",
    "        try:\n",
    "            if ('.' in value):\n",
    "                tmp = float(value)\n",
    "                if tmp < UPPER_BOUND:\n",
    "                    cds_float.append(tmp) \n",
    "        except:\n",
    "            pass\n",
    "       \n",
    "        #now dates\n",
    "        try:\n",
    "            date_tmp = pd.to_datetime(str(value))\n",
    "            #avoid the case where the hh:mm:ss is only found, and then pandas creates a date which is current day\n",
    "            if date_tmp.date() != datetime.today().date():\n",
    "                dates.append(str(date_tmp))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            \n",
    "    record_data = {}\n",
    "    record_data['verbs'] = verbs\n",
    "    record_data['nouns'] = nouns\n",
    "    record_data['cardinals'] = cds\n",
    "    record_data['products'] = products\n",
    "    record_data['companies'] = organisations\n",
    "    record_data['locations'] = locations\n",
    "    record_data['dates'] = dates\n",
    "    record_data['currency_values'] = dates\n",
    "    record_data['values_raw'] = values\n",
    "    record_data['tokens'] = words\n",
    "    \n",
    "\n",
    "\n",
    "    if len(cds_float) > 0:\n",
    "        record_data['max_value'] = max(cds_float)\n",
    "    else:\n",
    "        record_data['max_value'] = 0\n",
    "\n",
    "\n",
    "    \n",
    "    if print_output:\n",
    "         \n",
    "        print('Currency Linked Value: {}'.format(currency_values))\n",
    "        print('Raw Values: {}'.format(values))\n",
    "    #     print('Tagged Words: {}'.format(nltk_tagged_words))\n",
    "        print('Verbs: {}'.format(verbs))\n",
    "        print('Nouns: {}'.format(nouns))\n",
    "        print('Cardinal Values: {}'.format(cds))\n",
    "        print('Products {}'.format(products))\n",
    "        print('Companies {}'.format(organisations))\n",
    "        print('Locations {}'.format(locations))\n",
    "        print('Dates {}'.format(dates))\n",
    "        print('MAX Value {}'.format( record_data['max_value']))\n",
    "        print('\\n ### \\n')\n",
    "\n",
    "    return record_data\n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacey_enricher(global_vars, words_list):\n",
    "    string_of_words = ' '.join(map(str, words_list)) \n",
    "    \n",
    "    string_of_words = string_of_words\n",
    "#     print(string_of_words)\n",
    "    doc = global_vars['nlp'](string_of_words)\n",
    "    #returned with a list of entities (ents), which have k,v of (text, label_)\n",
    "#     print([(X.text, X.label_) for X in doc.ents])\n",
    "    return doc\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Data\n",
    "\n",
    "Using Textract's repsponse, we look at each word in our data, and store it with it's confidence scores. Then a series of data enrichment processes take place.\n",
    "\n",
    "Note, we use a filtering approach to ensure that we are not introducing random stop words.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Enrichment Finished. 200 Records processed, 3102 Tokens removed\n"
     ]
    }
   ],
   "source": [
    "def process_textract_responses(global_vars, data):\n",
    "    stop_words = global_vars['stop_words']\n",
    "    stop_words_nltk = stopwords.words('english')\n",
    "    ps = PorterStemmer()\n",
    "    exclude = set(string.punctuation)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    records_enriched = {}\n",
    "    removed_tokens = 0\n",
    "    for key,response in data.items():\n",
    "        line_data = []\n",
    "        confidences = []\n",
    "        for item in response[\"Blocks\"]:\n",
    "                if item[\"BlockType\"] == \"WORD\":\n",
    "                    \n",
    "                    #we need to normailise and remove punctuation.\n",
    "                    word = item[\"Text\"].lower().strip()\n",
    "                   \n",
    "                    \n",
    "    \n",
    "                    #now remove all punctuation\n",
    "                    word = word.translate(table)\n",
    "            \n",
    "                    word_stemmed = ps.stem(word)\n",
    "                \n",
    "                    #we have some oddities\n",
    "                    word =  ' '.join(i for i in word.split(' ') if not i.endswith('pm'))\n",
    "                    word  = ' '.join(i for i in word.split(' ') if not i.endswith('am'))\n",
    "                    \n",
    "                    if (word not in stop_words) and (word_stemmed not in stop_words) and (word_stemmed not in stop_words_nltk):\n",
    "                        #first stem the word\n",
    "                        \n",
    "                        #final some last checks\n",
    "                        if (word.isspace() == False) and (len(word) > 1):\n",
    "                        \n",
    "                            line_data.append(word_stemmed)\n",
    "                            confidences.append({'word':word_stemmed,'confidence':item['Confidence']})\n",
    "                    else:\n",
    "                        removed_tokens += 1\n",
    "                    \n",
    "        #now we have the data\n",
    "        record_data = process_textract_text(line_data, False)\n",
    "        record_data['word_confidences'] = confidences\n",
    "        records_enriched[key] = record_data\n",
    "    \n",
    "    print('Text Enrichment Finished. {} Records processed, {} Tokens removed'.format(len(records_enriched), removed_tokens)) \n",
    "    return records_enriched\n",
    "    \n",
    "records_enriched = process_textract_responses(global_vars, textract_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Non-unique Tokens 8466\n",
      "Unique Tokens 4680\n",
      "Top 10 Common Tokens ['000', 'ca', '1200', 'chicken', 'fri', '10', 'ny', '12', '600', '1000']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyse_records(records_enriched, word_cloud=False):\n",
    "    \n",
    "#     keyword_filter = set(['receipt','total','sum','check','subtotal','restaurant','please','ticket',\n",
    "#                           'guest','guests','table','tax','order','thank','you','not','included',\n",
    "#                          'pm','am','sub','tip','service','charge','change','cash','credit',\n",
    "#                          'server','sale','dine','gst','date','due','station','bar','food',\n",
    "#                          'city','please','print','reprint','ave','blvd','rd','road','seat',\n",
    "#                          'item','tel','tota','st','visa','mastercard','discount','gratuity','balance',\n",
    "#                          'add','visit'])\n",
    "    words_conf = []\n",
    "    dates = []\n",
    "    #raw data\n",
    "    for key,data in records_enriched.items():\n",
    "        \n",
    "#         print(data['max_value'])\n",
    "        words_conf = words_conf + data['word_confidences']\n",
    "        dates = dates + data['dates']\n",
    "        \n",
    "    #let's look at the distribution of confidence scores.    \n",
    "    df_words = pd.DataFrame(words_conf)\n",
    "    \n",
    "    print('Total Non-unique Tokens {}'.format(df_words.shape[0]))\n",
    "    print('Unique Tokens {}'.format(df_words['word'].unique().shape[0]))\n",
    "    n = 10\n",
    "    top_n_terms = df_words['word'].value_counts()[:n].index.tolist()\n",
    "    print('Top {} Common Tokens {}'.format(n, top_n_terms))\n",
    "\n",
    "    df_words.hist()\n",
    "\n",
    "    words_list_non_unique = df_words['word'].tolist()\n",
    "    \n",
    "    \n",
    "    #Simple word cloud to view our data.\n",
    "    if word_cloud:\n",
    "        string_of_words = ' '.join(map(str, words_list_non_unique)) \n",
    "        wordcloud = WordCloud(\n",
    "            width = 3000,\n",
    "            height = 2000,\n",
    "            background_color = 'black',\n",
    "            stopwords = STOPWORDS).generate(string_of_words)\n",
    "        fig = plt.figure(\n",
    "        figsize = (40, 30),\n",
    "        facecolor = 'k',\n",
    "        edgecolor = 'k')\n",
    "        plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.show()\n",
    "\n",
    "#         df_dates = pd.DataFrame(dates)\n",
    "#         df_dates.plot()\n",
    "\n",
    "    return df_words\n",
    "df_words = analyse_records(records_enriched, word_cloud=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chicken</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fri</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         confidence\n",
       "word               \n",
       "000              73\n",
       "ca               55\n",
       "1200             39\n",
       "chicken          38\n",
       "fri              37"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.groupby('word').count().sort_values('confidence',ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Co-Occurrence\n",
    "\n",
    "It might be quite nice to understand the co-occurance of items within receipts.\n",
    "\n",
    "For those who have worked in this space before, we can used SVD techniques to calculate the co-occurance, or libraries such as GloVe, which uses Vector Spaces of word representations in order to analyse the tokens. \n",
    "\n",
    "For this example, we're going to start off with a very simple counter based approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Token Co-Occurrences 58840\n",
      "('ca', 'fri') 8\n",
      "('parti', 'svrck') 7\n",
      "('ca', 'coke') 6\n",
      "('ca', 'beef') 6\n",
      "('chees', 'fri') 6\n",
      "('park', 'ny') 6\n",
      "('fri', 'rice') 6\n",
      "('ca', 'dinner') 6\n",
      "('ca', 'tender') 6\n",
      "('ice', 'tea') 6\n",
      "('beach', 'ca') 5\n",
      "('miami', 'fl') 5\n",
      "('ca', 'drink') 5\n",
      "('san', 'ca') 5\n",
      "('ca', 'id') 5\n",
      "('french', 'fri') 5\n",
      "('chicken', 'rice') 5\n",
      "('ny', 'coffe') 5\n",
      "('ca', 'custom') 5\n",
      "('new', 'york') 5\n",
      "('ca', 'coffe') 4\n",
      "('drive', 'thru') 4\n",
      "('settl', 'tender') 4\n",
      "('ny', 'sp') 4\n",
      "('diet', 'coke') 4\n",
      "('oak', 'ca') 4\n",
      "('ca', 'soda') 4\n",
      "('cafe', 'ca') 4\n",
      "('ca', 'tbl') 4\n",
      "('new', 'ny') 4\n",
      "('fri', 'onion') 4\n",
      "('taco', 'de') 4\n",
      "('jericho', 'ny') 4\n",
      "('ca', 'grill') 4\n",
      "('grill', 'fri') 4\n",
      "('avenu', 'side') 4\n",
      "('ny', 'side') 4\n",
      "('acct', 'auth') 4\n",
      "('ca', 'regular') 4\n",
      "('mexican', 'chicken') 4\n",
      "('chicken', 'subtota') 4\n",
      "('custom', 'copi') 4\n",
      "('ca', 'chicken') 4\n",
      "('ca', 'aid') 4\n",
      "('ny', 'chicken') 4\n",
      "('sauc', 'fri') 4\n",
      "('chicken', 'beef') 4\n",
      "('beach', 'fl') 4\n",
      "('ca', 'b11') 3\n",
      "('settl', 'creat') 3\n"
     ]
    }
   ],
   "source": [
    "def token_coocurance(records_enriched, combination_size = 2):\n",
    "    \n",
    "    cooc_counts = {}\n",
    "    for key,data in records_enriched.items():\n",
    "        \n",
    "        tokens = data['tokens']\n",
    "        cleaned = [ x for x in tokens if not x.isdigit() ]\n",
    "        #remove dups\n",
    "        cleaned =  list(dict.fromkeys(cleaned))\n",
    "        cooc = list(combinations(cleaned, combination_size)) \n",
    "        \n",
    "        for occ in cooc:\n",
    "            if occ in cooc_counts:\n",
    "                cnt = cooc_counts[occ]\n",
    "                cooc_counts[occ] = cnt +1\n",
    "            else:\n",
    "                cooc_counts[occ] = 1\n",
    "    \n",
    "    print('Total Unique Token Co-Occurrences {}'.format(len(cooc_counts)))\n",
    "    cooc_counts_sorted = dict( sorted(cooc_counts.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    top_k = 50\n",
    "    cnt = 0\n",
    "    for k,v in cooc_counts_sorted.items():\n",
    "        if cnt < top_k:\n",
    "            print(k, v)\n",
    "            cnt+= 1\n",
    "            \n",
    "\n",
    "token_coocurance(records_enriched, combination_size = 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
