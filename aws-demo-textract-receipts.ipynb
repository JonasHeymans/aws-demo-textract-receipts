{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Textract - Receipts Demo\n",
    "\n",
    "In the following notebook, we will examine the use of Amazon Textract in order to perform optical character recognition (OCR), and then use Amazon Machine Learning to organize the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Imports\n",
    "\n",
    "These are the libraries which we will require in order to complete different types of operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Set-up\n",
    "Configure all global constants here, these are variables which will remain constant throughout the execution of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: S3 bucket name must begin with \"deeplens-\" for DeepLens deployment\n",
    "bucket_name='aws-demo-receipts'\n",
    "prefix = '' #only use this if you want to have your files in a folder \n",
    "dataset_prefix = 'receipts_data'\n",
    "training_data_prefix = 'training_data'\n",
    "output_data = 'output_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Setting up the environment involves ensuring all the corret session and IAM roles are configured. We also need to ensure the correct region and bucket is made available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket already exists\n"
     ]
    }
   ],
   "source": [
    "def setup_env():\n",
    "    \n",
    "    role = get_execution_role()\n",
    "\n",
    "    sess = sagemaker.Session()\n",
    "\n",
    "    \n",
    "    AWS_REGION = 'us-east-1'\n",
    "    s3 = boto3.resource('s3')\n",
    "\n",
    "    s3_bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    if s3_bucket.creation_date == None:\n",
    "    # create S3 bucket because it does not exist yet\n",
    "        print('Creating S3 bucket {}.'.format(bucket))\n",
    "        resp = s3.create_bucket(\n",
    "            ACL='private',\n",
    "            Bucket=bucket\n",
    "        )\n",
    "    else:\n",
    "        print('Bucket already exists')\n",
    "    return role, sess, AWS_REGION, s3,s3_bucket\n",
    "\n",
    "role, sess,  AWS_REGION, s3, s3_bucket = setup_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data and Create Manifest\n",
    "\n",
    "Here we are goin to download the data to our bucket if it does not already exist. This dataset is a pre-compiled dataset of car images which contain 2 typers:\n",
    "\n",
    "- Whole (e.g. without damage)\n",
    "- Damaged (e.g. those with damage)\n",
    "\n",
    "The following create_dataset method will first download the Zip of the data, and then unpack it to the bucket named in the global constants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(bucket_name, s3_bucket):\n",
    "    \n",
    "    dataset_key = 'car-damage-dataset.zip'\n",
    "    objs = list(s3_bucket.objects.filter(Prefix=dataset_key))\n",
    "    if len(objs) > 0 and objs[0].key == dataset_key:\n",
    "        exists = True\n",
    "        print('{} Already Exists'.format(dataset_key) )\n",
    "    else:\n",
    "        exists = False\n",
    "    \n",
    "    if not exists:\n",
    "    \n",
    "        bucket = bucket_name\n",
    "        #copy first\n",
    "\n",
    "        source= { 'Bucket' : 'public-datasets', 'Key': dataset_key}\n",
    "        s3_bucket.copy(source, dataset_key)\n",
    "\n",
    "\n",
    "        s3 = boto3.client('s3', use_ssl=False)\n",
    "        Key_unzip = dataset_unpacked_dir\n",
    "\n",
    "    \n",
    "        s3_resource = boto3.resource('s3')\n",
    "        #Now create zip object one by one, this below is for 1st file in file_list\n",
    "        zip_obj = s3_resource.Object(bucket_name=bucket_name, key=dataset_key)\n",
    "        print('Unpacking {}\\n'.format(dataset_key))\n",
    "        \n",
    "        print (zip_obj)\n",
    "        buffer = BytesIO(zip_obj.get()[\"Body\"].read())\n",
    "        z = zipfile.ZipFile(buffer)\n",
    "        for filename in z.namelist():\n",
    "            file_info = z.getinfo(filename)\n",
    "            s3_resource.meta.client.upload_fileobj(\n",
    "                z.open(filename),\n",
    "                Bucket=bucket_name,\n",
    "                Key=Key_unzip + f'{filename}')\n",
    "            \n",
    "        \n",
    "    \n",
    "# create_dataset(bucket_name, s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
